задача классификации: надо определить к какому классу принадлежит объект

у тебя в варианте датасет CIFAR-10, там цветные картинки 32*32 пикселя
классы: car, bird, cat, deer, dog, frog, ship...

задача: дана новая картинка, определить ее в один из этих 10 классов.

kNN - k Nearest Neighbours
k ближайших соседей

мы представляем что картинки размещены в пространстве, и ищем ближайшие картинки к заданной

кажется логичным что если картинки находятся рядом, значит они похожи, значит они одного класса.

как мы их размещаем и в каком пространстве?
цветные картинки заданы пикселями, цвета пикселей это числа от 0 до 255.
мы записываем эти значения друг за другом и считаем их координатами картинки.
у тебя получится 32 * 32 * 3(цвет) = 3072 значения
то есть получится 3072-х размерное пространство. представить как оно выглядит невозможно

но принцип должен быть понятен

а расстояние между точками считаем как и в обычной геометрии

$$ L2 = \sqrt{ (x_1 - x_2)^2  + (y_1 - y_2)^2 } $$

$$ L2 = \sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2 + (w_1 - w_2)^2...} $$

такое расстояние называется L2, евклидова норма
L1, это манхетонское растояние, оно тут не использовалось

в итоге мы считаем расстояние от заданной картинки до всех остальных и смотрим на k ближайших.

в целом по kNN все.

Почему мы считаем координаты именно так? Почему мы считаем расстояние именно так?

Ответ: потому что.

Это такая упрощенная математическая абстракция.


Softmax

это такая смешная функция

$$  softmax(x) = \frac{e^{x_i}}{1\sum^K_{j=1}{e^{x_j}}}     $$

Забавное свойство этой функции - на выходе получаются значения в диапазоне [0,1], и их сумма равна 1.

Такими же свойствами обладает вероятность (из тервера!).

После применения этой функции мы получаем вектор значений (кол-во элементов по количеству классов(10)).

В итоге мы можем считать эти значения вероятностями того что объект принадлежит к классу 1, к классу 2, к классу 3, ...

Так как мы знаем правильный ответ для классификации(так как это тренировка), мы можем сказать софтмаксу правильно или неправильно он определил, и на сколько он ошибся.

и мы можем сказать как повлияли веса на алгоритм.

функция потерь  выглядит так:

$$ L = -\sum_i^K {t_i * log(softmax(x)_i)}  $$

$ t_i $ -  правильный ответ

получившийся *градиент* мы прибавляем к весам и повторяем обучение с новой картинкой.

Получившийся процесс:
1. Генерируем рандомные веса
2. Считаем X * w
3. Считаем softmax
4. Сравниваем с правильным ответом
5. Считаем производную softmax
6. обновляем веса

у этого метода точность выше чем у kNN.